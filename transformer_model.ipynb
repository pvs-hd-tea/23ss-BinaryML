{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:32:15.930990700Z",
     "start_time": "2023-06-04T21:32:15.883249200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Set the path to the downloaded EMBER dataset directory\n",
    "data_path = \"data/ember2018\"\n",
    "\n",
    "train_pickle_file = \"data/training_dump.pkl\"\n",
    "test_pickle_file = \"data/test_dump.pkl\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:32:15.934993400Z",
     "start_time": "2023-06-04T21:32:15.890375200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def printf(*args, **kwargs):\n",
    "    # Get the current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Add the timestamp to the message\n",
    "    message = f\"[{timestamp}] \" + ' '.join(map(str, args))\n",
    "\n",
    "    # Call the original print function with the modified message\n",
    "    print(message, **kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:32:16.023008100Z",
     "start_time": "2023-06-04T21:32:15.897386400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def load_features(file_path):\n",
    "    features = []\n",
    "    with jsonlines.open(os.path.join(data_path, file_path)) as reader:\n",
    "        for sample in reader:\n",
    "            features.append(sample)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:32:16.040993500Z",
     "start_time": "2023-06-04T21:32:15.906430500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MalwareDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sequence, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:32:16.041992300Z",
     "start_time": "2023-06-04T21:32:15.913982Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 23:32:15] Loading data from train_dump.pkl...\n",
      "[2023-06-04 23:32:46] Completed loading training feature data, train_features1.len() =  182182\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(train_pickle_file):\n",
    "    printf(\"Loading data from train_dump.pkl...\")\n",
    "    # Load the train_features from the pickle file\n",
    "    with open(train_pickle_file, \"rb\") as f:\n",
    "        train_features1 = pickle.load(f)\n",
    "else:\n",
    "    # Load the train_features from the JSONL files\n",
    "    train_features1 = []\n",
    "    for i in {0, 4}:\n",
    "        printf(\"Collecting data from train_features1_\" + str(i) + \".jsonl...\")\n",
    "        train_features1.extend(load_features(\"train_features_\" + str(i) + \".jsonl\"))\n",
    "\n",
    "    # Save the train_features to the pickle file\n",
    "    with open(train_pickle_file, \"wb\") as f:\n",
    "        pickle.dump(train_features1, f)\n",
    "printf(\"Completed loading training feature data, train_features1.len() = \", len(train_features1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:32:46.634331Z",
     "start_time": "2023-06-04T21:32:15.931992900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 23:32:46] Loading data from test_dump.pkl...\n",
      "[2023-06-04 23:35:51] Completed loading test feature data:  200000\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(test_pickle_file):\n",
    "    printf(\"Loading data from test_dump.pkl...\")\n",
    "    # Load the test_features from the pickle file\n",
    "    with open(test_pickle_file, \"rb\") as f:\n",
    "        test_features = pickle.load(f)\n",
    "else:\n",
    "    printf(\"Collecting data from test_features.jsonl...\")\n",
    "    # Load the test_features from the JSONL file\n",
    "    test_features = load_features(\"test_features.jsonl\")\n",
    "\n",
    "    # Save the test_features to the pickle file\n",
    "    with open(test_pickle_file, \"wb\") as f:\n",
    "        pickle.dump(test_features, f)\n",
    "printf(\"Completed loading test feature data: \", len(test_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:35:51.252528500Z",
     "start_time": "2023-06-04T21:32:46.635332200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape: 145745\n",
      "Train labels shape: 145745\n",
      "val_labels shape: 36437\n",
      "val_features shape: 36437\n",
      "test_labels shape: 200000\n",
      "[2023-06-04 23:35:51] Training set class distribution: {-1: 28334, 0: 78835, 1: 38576}\n",
      "[2023-06-04 23:35:51] Validation set class distribution: {-1: 7002, 0: 19598, 1: 9837}\n",
      "[2023-06-04 23:35:51] Testing set class distribution: {0: 100000, 1: 100000}\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_features, val_features = train_test_split(train_features1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract labels from the features\n",
    "try:\n",
    "    train_labels = [sample['label'] for sample in train_features]\n",
    "    val_labels = [sample['label'] for sample in val_features]\n",
    "except:\n",
    "    train_labels = [sample['label'] for sample in train_features[0]]\n",
    "    val_labels = [sample['label'] for sample in val_features[0]]\n",
    "test_labels = [sample['label'] for sample in test_features]\n",
    "\n",
    "print(\"train_features shape:\", len(train_features))\n",
    "print(\"Train labels shape:\", len(train_labels))\n",
    "print(\"val_labels shape:\", len(val_labels))\n",
    "print(\"val_features shape:\", len(val_features))\n",
    "print(\"test_labels shape:\", len(test_labels))\n",
    "\n",
    "# Check the class distribution in the training, validation, and testing sets\n",
    "train_class_counts = dict(zip(*np.unique(train_labels, return_counts=True)))\n",
    "val_class_counts = dict(zip(*np.unique(val_labels, return_counts=True)))\n",
    "test_class_counts = dict(zip(*np.unique(test_labels, return_counts=True)))\n",
    "\n",
    "printf(\"Training set class distribution:\", train_class_counts)\n",
    "printf(\"Validation set class distribution:\", val_class_counts)\n",
    "printf(\"Testing set class distribution:\", test_class_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:35:52.004783200Z",
     "start_time": "2023-06-04T21:35:51.244554100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Determine the dimensionality of numerical features\n",
    "numerical_dim = 100  # Replace with the actual dimensionality\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:35:52.452707200Z",
     "start_time": "2023-06-04T21:35:52.006784400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Encode numerical features\n",
    "def encode_numerical_features(features):\n",
    "    encoded_features = []\n",
    "    for sample in features:\n",
    "        encoded_sample = []\n",
    "        for feature_name, feature_value in sample.items():\n",
    "            if isinstance(feature_value, float) or isinstance(feature_value, int):\n",
    "                # Normalize numerical values to [0, 1] range\n",
    "                encoded_sample.append(feature_value / numerical_dim)\n",
    "        encoded_features.append(encoded_sample)\n",
    "    return np.array(encoded_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:35:52.461050400Z",
     "start_time": "2023-06-04T21:35:52.460034600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Convert features to sequential format\n",
    "def convert_to_sequences(features, tokenizer):\n",
    "    sequences = []\n",
    "    for sample in features:\n",
    "        sequence = []\n",
    "        for feature_name, feature_value in sample.items():\n",
    "            if isinstance(feature_value, str):\n",
    "                # Tokenize string features\n",
    "                tokens = tokenizer.tokenize(feature_value)\n",
    "                sequence.extend(tokens)\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:35:52.465970500Z",
     "start_time": "2023-06-04T21:35:52.463077Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 23:41:32] Training set sequence encoding: 145745\n",
      "[2023-06-04 23:42:00] Validation set sequence encoding: 36437\n",
      "[2023-06-04 23:44:34] Test set sequence encoding: 200000\n",
      "[2023-06-04 23:44:34] Training set sequence class distribution: {-1: 28334, 0: 78835, 1: 38576}\n",
      "[2023-06-04 23:44:34] Validation set sequence class distribution: {-1: 7002, 0: 19598, 1: 9837}\n",
      "[2023-06-04 23:44:34] Testing set sequence class distribution: {0: 100000, 1: 100000}\n"
     ]
    }
   ],
   "source": [
    "# Encode numerical features in train, validation, and test sets\n",
    "train_encoded = encode_numerical_features(train_features)\n",
    "val_encoded = encode_numerical_features(val_features)\n",
    "test_encoded = encode_numerical_features(test_features)\n",
    "\n",
    "# Convert features to sequential format\n",
    "train_sequences = convert_to_sequences(train_features, tokenizer)\n",
    "printf(\"Training set sequence encoding:\", len(train_sequences))\n",
    "val_sequences = convert_to_sequences(val_features, tokenizer)\n",
    "printf(\"Validation set sequence encoding:\", len(val_sequences))\n",
    "test_sequences = convert_to_sequences(test_features, tokenizer)\n",
    "printf(\"Test set sequence encoding:\", len(test_sequences))\n",
    "\n",
    "# Check the class distribution in the sequence data\n",
    "train_sequence_class_counts = dict(zip(*np.unique(train_labels, return_counts=True)))\n",
    "val_sequence_class_counts = dict(zip(*np.unique(val_labels, return_counts=True)))\n",
    "test_sequence_class_counts = dict(zip(*np.unique(test_labels, return_counts=True)))\n",
    "\n",
    "printf(\"Training set sequence class distribution:\", train_sequence_class_counts)\n",
    "printf(\"Validation set sequence class distribution:\", val_sequence_class_counts)\n",
    "printf(\"Testing set sequence class distribution:\", test_sequence_class_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:44:34.322776700Z",
     "start_time": "2023-06-04T21:35:52.469932800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "# Load pre-trained model\n",
    "model_name = \"bert-base-uncased\"\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, config=config)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:44:35.787836900Z",
     "start_time": "2023-06-04T21:44:34.335008400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 23:44:35] Converting train_strings sequences back to string representation...\n",
      "[2023-06-04 23:44:49] Converting val_strings sequences back to string representation...\n",
      "[2023-06-04 23:44:52] Converting test_strings sequences back to string representation...\n",
      "[2023-06-04 23:45:10] Tokenizing the train_encodings sequences...\n",
      "[2023-06-04 23:46:58] Tokenizing the val_encodings sequences...\n",
      "[2023-06-04 23:47:24] Tokenizing the test_encodings sequences...\n",
      "[2023-06-04 23:55:09] Converting the encodings to tensors...\n",
      "Train labels shape: 145745\n",
      "Val labels  shape: 36437\n",
      "Test labels shape: 200000\n",
      "[2023-06-04 23:55:09] Converting the labels to tensors...\n",
      "[2023-06-04 23:55:09] Done converting the labels to tensors!\n",
      "Train labels shape: 145745\n",
      "Val labels  shape: 36437\n",
      "Test labels shape: 200000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert sequences back to string representation\n",
    "printf(\"Converting train_strings sequences back to string representation...\")\n",
    "train_strings = [tokenizer.decode(tokenizer.convert_tokens_to_ids(seq), skip_special_tokens=False) for seq in train_sequences]\n",
    "printf(\"Converting val_strings sequences back to string representation...\")\n",
    "val_strings = [tokenizer.decode(tokenizer.convert_tokens_to_ids(seq), skip_special_tokens=False) for seq in val_sequences]\n",
    "printf(\"Converting test_strings sequences back to string representation...\")\n",
    "test_strings = [tokenizer.decode(tokenizer.convert_tokens_to_ids(seq), skip_special_tokens=False) for seq in test_sequences]\n",
    "\n",
    "# Tokenize the sequences\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "printf(\"Tokenizing the train_encodings sequences...\")\n",
    "train_encodings = tokenizer.batch_encode_plus(train_strings, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "printf(\"Tokenizing the val_encodings sequences...\")\n",
    "val_encodings = tokenizer.batch_encode_plus(val_strings, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "printf(\"Tokenizing the test_encodings sequences...\")\n",
    "test_encodings = tokenizer.batch_encode_plus(test_strings, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "printf(\"Converting the encodings to tensors...\")\n",
    "# Convert the encodings to tensors\n",
    "train_input_ids = train_encodings[\"input_ids\"]\n",
    "train_attention_mask = train_encodings[\"attention_mask\"]\n",
    "val_input_ids = val_encodings[\"input_ids\"]\n",
    "val_attention_mask = val_encodings[\"attention_mask\"]\n",
    "test_input_ids = test_encodings[\"input_ids\"]\n",
    "test_attention_mask = test_encodings[\"attention_mask\"]\n",
    "\n",
    "print(\"Train labels shape:\", len(train_labels))\n",
    "print(\"Val labels  shape:\", len(val_labels))\n",
    "print(\"Test labels shape:\", len(test_labels))\n",
    "\n",
    "printf(\"Converting the labels to tensors...\")\n",
    "# Convert the labels to tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "printf(\"Done converting the labels to tensors!\")\n",
    "print(\"Train labels shape:\", len(train_labels))\n",
    "print(\"Val labels  shape:\", len(val_labels))\n",
    "print(\"Test labels shape:\", len(test_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:55:09.846211300Z",
     "start_time": "2023-06-04T21:44:35.778190100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input ids shape: torch.Size([145745, 90])\n",
      "Train attention mask shape: torch.Size([145745, 90])\n",
      "Train labels shape: torch.Size([145745])\n",
      "Val input ids shape: torch.Size([36437, 90])\n",
      "Val attention mask shape: torch.Size([36437, 90])\n",
      "Val labels shape: torch.Size([36437])\n",
      "Test input ids shape: torch.Size([200000, 90])\n",
      "Test attention mask shape: torch.Size([200000, 90])\n",
      "Test labels shape: torch.Size([200000])\n",
      "Train labels shape: torch.Size([145745])\n",
      "Val labels shape: torch.Size([36437])\n",
      "Test labels shape: torch.Size([200000])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train input ids shape:\", train_input_ids.shape)\n",
    "print(\"Train attention mask shape:\", train_attention_mask.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "\n",
    "print(\"Val input ids shape:\", val_input_ids.shape)\n",
    "print(\"Val attention mask shape:\", val_attention_mask.shape)\n",
    "print(\"Val labels shape:\", val_labels.shape)\n",
    "\n",
    "print(\"Test input ids shape:\", test_input_ids.shape)\n",
    "print(\"Test attention mask shape:\", test_attention_mask.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Check the shape of the label tensors\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Val labels shape:\", val_labels.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n",
    "\n",
    "# Define the number of classes excluding the unlabeled class (-1)\n",
    "num_classes = 2  # Set the number of classes for your classification task\n",
    "\n",
    "# Filter out unlabeled instances (-1)\n",
    "train_mask = train_labels != -1\n",
    "val_mask = val_labels != -1\n",
    "test_mask = test_labels != -1\n",
    "\n",
    "# Apply the masks to exclude unlabeled instances\n",
    "train_input_ids = train_input_ids[train_mask]\n",
    "train_attention_mask = train_attention_mask[train_mask]\n",
    "train_labels = train_labels[train_mask]\n",
    "val_input_ids = val_input_ids[val_mask]\n",
    "val_attention_mask = val_attention_mask[val_mask]\n",
    "val_labels = val_labels[val_mask]\n",
    "test_input_ids = test_input_ids[test_mask]\n",
    "test_attention_mask = test_attention_mask[test_mask]\n",
    "test_labels = test_labels[test_mask]\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "train_labels_onehot = torch.nn.functional.one_hot(train_labels, num_classes=num_classes).float()\n",
    "val_labels_onehot = torch.nn.functional.one_hot(val_labels, num_classes=num_classes).float()\n",
    "test_labels_onehot = torch.nn.functional.one_hot(test_labels, num_classes=num_classes).float()\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels_onehot)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels_onehot)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels_onehot)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 4  # Set the batch size to your desired value\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:55:12.381997100Z",
     "start_time": "2023-06-04T21:55:09.705739600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class MalwareDetectionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MalwareDetectionModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1))  # Reshape input_ids tensor\n",
    "        embedded = self.embedding(input_ids.to(torch.float))\n",
    "        embedded = self.dropout(embedded)\n",
    "        logits = self.classifier(embedded)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:55:12.387999900Z",
     "start_time": "2023-06-04T21:55:10.565304700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Val Loss: 217.8138, Val Accuracy: 0.7280\n",
      "Epoch 2/10: Val Loss: 301.6873, Val Accuracy: 0.5559\n",
      "Epoch 3/10: Val Loss: 494.2894, Val Accuracy: 0.6829\n",
      "Epoch 4/10: Val Loss: 173.3081, Val Accuracy: 0.7219\n",
      "Epoch 5/10: Val Loss: 194.5277, Val Accuracy: 0.6797\n",
      "Epoch 6/10: Val Loss: 211.1851, Val Accuracy: 0.6740\n",
      "Epoch 7/10: Val Loss: 278.2798, Val Accuracy: 0.6969\n",
      "Epoch 8/10: Val Loss: 152.6165, Val Accuracy: 0.6465\n",
      "Epoch 9/10: Val Loss: 354.1522, Val Accuracy: 0.7138\n",
      "Epoch 10/10: Val Loss: 197.8352, Val Accuracy: 0.7544\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "input_size = 90  # Set the input size according to your data\n",
    "hidden_size = 768  # Adjust the hidden size as per your requirements\n",
    "num_classes = 2  # Adjust the number of classes as per your requirements\n",
    "\n",
    "model = MalwareDetectionModel(input_size, hidden_size, num_classes)\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "learning_rate = 0.001  # Set your desired learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  # Set your desired batch size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Set your desired number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(val_dataloader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:59:15.013475900Z",
     "start_time": "2023-06-04T21:55:10.594607100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 278.8567 - Test Accuracy: 0.6529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing on test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted_labels = torch.max(logits, 1)\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "test_loss = test_loss / len(test_dataloader)\n",
    "test_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T21:59:23.042154500Z",
     "start_time": "2023-06-04T21:59:15.028392100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6529\n",
      "Precision: 0.7006\n",
      "Recall: 0.6529\n",
      "F1-score: 0.6310\n",
      "AUC-ROC: 0.6529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "        predictions.extend(predicted_labels.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "auc_roc = roc_auc_score(true_labels, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T22:32:11.584285200Z",
     "start_time": "2023-06-04T22:32:02.850295500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
